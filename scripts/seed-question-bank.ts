/**
 * Jarre - Seed Question Bank
 *
 * ~270 curated factual questions across 66 concepts.
 * Generated by Claude 4.6, not via DeepSeek API.
 *
 * Run with: npx tsx scripts/seed-question-bank.ts
 *
 * Requires SUPABASE_URL and SUPABASE_SERVICE_ROLE_KEY in .env.local
 */

import { createClient } from '@supabase/supabase-js';
import * as dotenv from 'dotenv';
import { resolve } from 'path';

dotenv.config({ path: resolve(__dirname, '../.env.local') });

const supabaseUrl = process.env.NEXT_PUBLIC_SUPABASE_URL;
const supabaseKey = process.env.SUPABASE_SECRET_KEY;

if (!supabaseUrl || !supabaseKey) {
  console.error('Missing NEXT_PUBLIC_SUPABASE_URL or SUPABASE_SECRET_KEY in .env.local');
  process.exit(1);
}

const supabase = createClient(supabaseUrl, supabaseKey);

// ============================================================================
// TYPES
// ============================================================================

interface SeedQuestion {
  concept_id: string;
  type: 'definition' | 'fact' | 'property' | 'guarantee' | 'complexity' | 'comparison';
  question_text: string;
  expected_answer: string;
  difficulty: 1 | 2 | 3;
  related_concept_id?: string;
}

// ============================================================================
// QUESTIONS
// ============================================================================

const questions: SeedQuestion[] = [
  // ==========================================================================
  // PHASE 1: Distributed Systems Fundamentals
  // ==========================================================================

  // --- reliability ---
  {
    concept_id: 'reliability',
    type: 'definition',
    question_text: '¿Qué significa que un sistema sea "reliable"? ¿Es lo mismo que ser "fault-free"?',
    expected_answer: 'Un sistema reliable sigue funcionando correctamente incluso cuando ocurren fallas (faults). No es lo mismo que fault-free: un sistema reliable es fault-tolerant, acepta que las fallas ocurren y las maneja.',
    difficulty: 1,
  },
  {
    concept_id: 'reliability',
    type: 'property',
    question_text: '¿Cuáles son los tres tipos principales de faults que afectan la reliability?',
    expected_answer: 'Hardware faults (disco falla, RAM corrupta), software faults (bugs, cascading failures), y faults humanos (errores de configuración, deploys malos). Los faults humanos son la causa más común.',
    difficulty: 2,
  },
  {
    concept_id: 'reliability',
    type: 'fact',
    question_text: '¿Cuál es la diferencia entre fault, error y failure?',
    expected_answer: 'Un fault es el componente que se desvía de su spec. Un error es el estado incorrecto del sistema causado por un fault. Un failure es cuando el sistema como un todo deja de proveer el servicio esperado al usuario.',
    difficulty: 2,
  },
  {
    concept_id: 'reliability',
    type: 'comparison',
    question_text: '¿En qué se diferencia la reliability de la availability?',
    expected_answer: 'Reliability es la probabilidad de que el sistema funcione correctamente durante un período. Availability es la fracción de tiempo que el sistema está operativo. Un sistema puede estar available (responde) pero no reliable (responde incorrectamente).',
    difficulty: 2,
    related_concept_id: 'slos-slis',
  },

  // --- scalability ---
  {
    concept_id: 'scalability',
    type: 'definition',
    question_text: '¿Qué es scalability y cómo se mide?',
    expected_answer: 'Scalability es la capacidad de un sistema de manejar carga creciente agregando recursos. Se mide en términos de load parameters específicos al sistema: requests/segundo, volumen de datos, usuarios concurrentes, etc.',
    difficulty: 1,
  },
  {
    concept_id: 'scalability',
    type: 'property',
    question_text: '¿Cuál es la diferencia entre scaling vertical y horizontal?',
    expected_answer: 'Scaling vertical (scale up) es usar una máquina más potente. Scaling horizontal (scale out) es distribuir la carga entre múltiples máquinas. Horizontal es más flexible pero introduce complejidad de sistemas distribuidos.',
    difficulty: 1,
  },
  {
    concept_id: 'scalability',
    type: 'fact',
    question_text: '¿Qué son los percentiles de latencia y por qué p99 importa más que el promedio?',
    expected_answer: 'Los percentiles muestran la distribución de tiempos de respuesta. p99 indica que el 99% de requests son más rápidos que ese valor. Importa más que el promedio porque los usuarios más lentos suelen ser los que tienen más datos (más valiosos) y porque un request lento en un servicio fan-out afecta toda la respuesta.',
    difficulty: 2,
  },
  {
    concept_id: 'scalability',
    type: 'comparison',
    question_text: '¿Cuál es la diferencia entre throughput y latency como load parameters?',
    expected_answer: 'Throughput es la cantidad de trabajo procesada por unidad de tiempo (requests/seg, records/seg). Latency es el tiempo que tarda un request individual. Pueden ser inversamente correlacionados: optimizar para throughput (batching) puede aumentar la latency individual.',
    difficulty: 2,
    related_concept_id: 'tail-latency',
  },

  // --- maintainability ---
  {
    concept_id: 'maintainability',
    type: 'definition',
    question_text: '¿Cuáles son los tres principios de diseño de la maintainability según DDIA?',
    expected_answer: 'Operability (fácil de operar para el equipo de ops), Simplicity (manejo de complejidad, abstracciones claras), y Evolvability (facilidad de hacer cambios futuros, también llamada extensibility).',
    difficulty: 1,
  },
  {
    concept_id: 'maintainability',
    type: 'property',
    question_text: '¿Por qué la simplicidad es crucial para la maintainability de un sistema?',
    expected_answer: 'La complejidad accidental dificulta entender el sistema, aumenta bugs y hace los cambios más riesgosos. La simplicidad se logra con buenas abstracciones que esconden detalles de implementación sin ocultar semántica importante.',
    difficulty: 2,
  },

  // --- data-models ---
  {
    concept_id: 'data-models',
    type: 'definition',
    question_text: '¿Cuáles son los tres modelos de datos principales y qué operaciones facilita cada uno?',
    expected_answer: 'Relacional: bueno para joins, datos normalizados, relaciones many-to-many. Document: bueno para datos jerárquicos, self-contained, schema flexible. Graph: bueno para relaciones complejas y queries de conectividad (N grados de separación).',
    difficulty: 1,
  },
  {
    concept_id: 'data-models',
    type: 'property',
    question_text: '¿Qué es el impedance mismatch entre modelo de datos y código de aplicación?',
    expected_answer: 'Es la desconexión entre cómo se almacenan los datos (tablas relacionales) y cómo se usan en código (objetos/structs). Los ORMs intentan resolver esto pero agregan complejidad. Los document DBs reducen el mismatch para datos jerárquicos.',
    difficulty: 2,
  },
  {
    concept_id: 'data-models',
    type: 'comparison',
    question_text: '¿Cuándo conviene un modelo de documentos vs relacional?',
    expected_answer: 'Documentos: cuando los datos son self-contained (un perfil de usuario completo), se acceden juntos, y no hay muchos joins entre documentos. Relacional: cuando hay muchas relaciones many-to-many, se necesitan joins complejos, o los datos se acceden de formas impredecibles.',
    difficulty: 2,
    related_concept_id: 'storage-engines',
  },

  // --- storage-engines ---
  {
    concept_id: 'storage-engines',
    type: 'definition',
    question_text: '¿Cuáles son los dos tipos principales de storage engines y su trade-off fundamental?',
    expected_answer: 'Log-structured (LSM trees): optimizados para writes, append-only. Page-oriented (B-trees): optimizados para reads, in-place updates. El trade-off es write amplification vs read amplification.',
    difficulty: 1,
  },
  {
    concept_id: 'storage-engines',
    type: 'property',
    question_text: '¿Qué es un LSM tree y cómo funciona la compactación?',
    expected_answer: 'LSM tree escribe primero a un memtable en memoria, luego flush a disco como SSTable ordenada. La compactación merge-sortea múltiples SSTables para eliminar duplicados y deletes. Trade-off: mejor write throughput pero reads pueden necesitar buscar en múltiples SSTables.',
    difficulty: 2,
  },
  {
    concept_id: 'storage-engines',
    type: 'complexity',
    question_text: '¿Cuál es la complejidad de búsqueda en un B-tree vs un LSM tree?',
    expected_answer: 'B-tree: O(log n) para lectura, garantizado. LSM tree: O(log n) por cada SSTable, y puede haber múltiples niveles de SSTables. En el peor caso, LSM lee de todos los niveles, pero Bloom filters mitigan esto significativamente.',
    difficulty: 3,
  },
  {
    concept_id: 'storage-engines',
    type: 'fact',
    question_text: '¿Qué es write amplification y por qué importa?',
    expected_answer: 'Write amplification es la ratio entre bytes escritos a disco vs bytes escritos por la aplicación. En B-trees, un update puede reescribir una página entera. En LSM trees, la compactación reescribe datos múltiples veces. Afecta el desgaste de SSDs y throughput.',
    difficulty: 2,
  },

  // --- replication ---
  {
    concept_id: 'replication',
    type: 'definition',
    question_text: '¿Cuáles son los tres modelos principales de replicación?',
    expected_answer: 'Leader-based (single leader): un nodo acepta writes, los réplicas copian. Multi-leader: múltiples nodos aceptan writes. Leaderless: cualquier nodo acepta writes, se usan quorums para consistencia.',
    difficulty: 1,
  },
  {
    concept_id: 'replication',
    type: 'property',
    question_text: '¿Qué es la replicación síncrona vs asíncrona y sus trade-offs?',
    expected_answer: 'Síncrona: el leader espera confirmación del follower antes de confirmar al cliente. Garantiza durabilidad pero aumenta latencia. Asíncrona: el leader confirma sin esperar followers. Menor latencia pero riesgo de data loss si el leader falla.',
    difficulty: 2,
  },
  {
    concept_id: 'replication',
    type: 'fact',
    question_text: '¿Qué problemas causa el replication lag en sistemas eventualmente consistentes?',
    expected_answer: 'Read-after-write inconsistency (escribes algo y no lo ves al releer), monotonic read violations (lees un valor nuevo y luego uno viejo), y consistent prefix violations (ves efectos antes de sus causas).',
    difficulty: 2,
  },
  {
    concept_id: 'replication',
    type: 'comparison',
    question_text: '¿Cuándo usarías multi-leader vs single-leader replication?',
    expected_answer: 'Multi-leader cuando hay múltiples datacenters (latencia de escritura), apps offline-first (cada dispositivo es un leader), o collaborative editing. Single-leader para la mayoría de los casos: más simple, sin conflictos de escritura. Multi-leader requiere conflict resolution.',
    difficulty: 3,
    related_concept_id: 'consistency-models',
  },

  // --- partitioning ---
  {
    concept_id: 'partitioning',
    type: 'definition',
    question_text: '¿Qué es partitioning y cuáles son las dos estrategias principales?',
    expected_answer: 'Partitioning (sharding) divide un dataset grande entre múltiples máquinas. Estrategias: por key range (permite range queries eficientes pero riesgo de hot spots) y por hash (distribución más uniforme pero pierde orden).',
    difficulty: 1,
  },
  {
    concept_id: 'partitioning',
    type: 'property',
    question_text: '¿Qué es un hot spot en partitioning y cómo se mitiga?',
    expected_answer: 'Un hot spot es una partición que recibe desproporcionadamente más tráfico. Se mitiga con hash partitioning, splitting de particiones, o agregando un random suffix a keys populares para distribuir la carga.',
    difficulty: 2,
  },
  {
    concept_id: 'partitioning',
    type: 'fact',
    question_text: '¿Qué problema introduce el rebalancing de particiones?',
    expected_answer: 'Al redistribuir datos entre nodos, se genera tráfico de red y I/O masivo. Técnicas: fixed number of partitions, dynamic splitting, proportional to nodes. Importante: no usar hash mod N porque mover un nodo redistribuye casi todos los datos.',
    difficulty: 2,
  },

  // --- distributed-failures ---
  {
    concept_id: 'distributed-failures',
    type: 'definition',
    question_text: '¿Qué tipos de fallas son característicos de los sistemas distribuidos?',
    expected_answer: 'Network partitions (nodos no pueden comunicarse), node crashes, clock skew (relojes desincronizados), Byzantine faults (nodos que mienten). La diferencia clave: las fallas parciales son la norma, no la excepción.',
    difficulty: 1,
  },
  {
    concept_id: 'distributed-failures',
    type: 'property',
    question_text: '¿Por qué no se puede distinguir entre un nodo caído y uno con red lenta?',
    expected_answer: 'En una red asíncrona, no hay límite de tiempo garantizado para la entrega de mensajes. Un timeout solo indica que no hubo respuesta a tiempo, no si el nodo procesó el request, lo ignoró, o nunca lo recibió.',
    difficulty: 2,
  },
  {
    concept_id: 'distributed-failures',
    type: 'fact',
    question_text: '¿Qué es un Byzantine fault y cuándo importa?',
    expected_answer: 'Es cuando un nodo actúa de forma arbitraria o maliciosa (envía respuestas incorrectas). Importa en sistemas donde no confías en todos los participantes: blockchain, aviación, sistemas financieros. Para la mayoría de sistemas web, basta asumir crash-stop faults.',
    difficulty: 3,
  },

  // --- consistency-models ---
  {
    concept_id: 'consistency-models',
    type: 'definition',
    question_text: '¿Qué es linearizability y por qué es considerada "consistencia fuerte"?',
    expected_answer: 'Linearizability garantiza que una vez que un write completa, todos los reads posteriores ven ese valor. Es como si hubiera una sola copia de los datos. Es fuerte porque da las mismas garantías que un sistema no distribuido.',
    difficulty: 2,
  },
  {
    concept_id: 'consistency-models',
    type: 'fact',
    question_text: '¿Qué dice el teorema CAP?',
    expected_answer: 'Ante una network partition (P), debes elegir entre Consistency (C, todos los reads ven el write más reciente) y Availability (A, todos los nodos responden). No puedes tener ambas durante una partición. En la práctica, P es inevitable, así que la elección real es CP vs AP.',
    difficulty: 1,
  },
  {
    concept_id: 'consistency-models',
    type: 'comparison',
    question_text: '¿Cuál es la diferencia entre causal consistency y eventual consistency?',
    expected_answer: 'Eventual consistency solo garantiza que eventualmente todos verán el mismo valor, sin orden. Causal consistency preserva el orden de operaciones causalmente relacionadas: si A causó B, todos ven A antes que B. Causal es más fuerte que eventual pero más débil que linearizability.',
    difficulty: 3,
    related_concept_id: 'replication',
  },

  // --- consensus ---
  {
    concept_id: 'consensus',
    type: 'definition',
    question_text: '¿Qué problema resuelve el consenso en sistemas distribuidos?',
    expected_answer: 'Que múltiples nodos se pongan de acuerdo sobre un valor. Se usa para leader election, atomic commit, total order broadcast. Imposible de resolver en presencia de fallas arbitrarias (FLP impossibility en sistemas asíncronos).',
    difficulty: 1,
  },
  {
    concept_id: 'consensus',
    type: 'fact',
    question_text: '¿Cómo funciona Raft a alto nivel?',
    expected_answer: 'Raft elige un leader mediante elecciones con timeouts. El leader recibe todos los writes y los replica al log de los followers. Un write se compromete cuando la mayoría confirma. Si el leader falla, un follower con timeout dispara nueva elección.',
    difficulty: 2,
  },
  {
    concept_id: 'consensus',
    type: 'property',
    question_text: '¿Por qué los algoritmos de consenso requieren una mayoría (quorum)?',
    expected_answer: 'Porque dos quorums siempre se solapan en al menos un nodo, garantizando que la información más reciente está presente. Con N nodos, el quorum es floor(N/2)+1. Tolera hasta floor((N-1)/2) fallas.',
    difficulty: 2,
  },

  // --- stream-processing ---
  {
    concept_id: 'stream-processing',
    type: 'definition',
    question_text: '¿Qué es stream processing y en qué se diferencia del batch processing?',
    expected_answer: 'Stream processing procesa datos continuamente a medida que llegan, con baja latencia. Batch processing acumula datos y los procesa juntos periódicamente. Stream: latencia baja, procesamiento continuo. Batch: throughput alto, procesamiento completo del dataset.',
    difficulty: 1,
  },
  {
    concept_id: 'stream-processing',
    type: 'property',
    question_text: '¿Cuál es la diferencia entre event time y processing time en streams?',
    expected_answer: 'Event time es cuando el evento ocurrió (timestamp del evento). Processing time es cuando el sistema lo procesa. La diferencia importa para windowing correcto: eventos pueden llegar tarde o fuera de orden. Los sistemas robustos usan event time con watermarks para manejar late arrivals.',
    difficulty: 2,
  },
  {
    concept_id: 'stream-processing',
    type: 'guarantee',
    question_text: '¿Qué significa "exactly-once" semantics en stream processing y es realmente posible?',
    expected_answer: 'Exactly-once garantiza que cada evento se procese exactamente una vez, sin duplicados ni pérdida. En la práctica se logra con idempotency + at-least-once delivery, o transacciones distribuidas. No es "magia": requiere que el sistema completo (source, processor, sink) coopere.',
    difficulty: 3,
  },

  // --- slos-slis ---
  {
    concept_id: 'slos-slis',
    type: 'definition',
    question_text: '¿Cuál es la diferencia entre SLI, SLO y SLA?',
    expected_answer: 'SLI (Service Level Indicator): la métrica que mides (ej: latencia p99). SLO (Service Level Objective): el target para esa métrica (ej: p99 < 200ms). SLA (Service Level Agreement): contrato con consecuencias si no se cumple el SLO.',
    difficulty: 1,
  },
  {
    concept_id: 'slos-slis',
    type: 'property',
    question_text: '¿Qué es un error budget y cómo se usa?',
    expected_answer: 'Es la cantidad de downtime o errores permitidos según el SLO. Si el SLO es 99.9% disponibilidad, el error budget es 0.1% (43 min/mes). Si queda budget, puedes hacer cambios riesgosos (deploys). Si se agota, priorizas estabilidad.',
    difficulty: 2,
  },

  // --- monitoring ---
  {
    concept_id: 'monitoring',
    type: 'definition',
    question_text: '¿Cuáles son las cuatro señales doradas del monitoring según Google SRE?',
    expected_answer: 'Latency (tiempo de respuesta), Traffic (demanda al sistema), Errors (tasa de respuestas fallidas), y Saturation (cuán lleno está el recurso más limitado). Si monitoreas estas cuatro, cubres la mayoría de problemas.',
    difficulty: 1,
  },
  {
    concept_id: 'monitoring',
    type: 'comparison',
    question_text: '¿Cuál es la diferencia entre metrics, logs y traces?',
    expected_answer: 'Metrics: valores numéricos agregados en el tiempo (CPU %, requests/sec). Logs: eventos discretos detallados (texto). Traces: el recorrido de un request a través de múltiples servicios. Los tres son complementarios: metrics para alertar, logs para investigar, traces para diagnosticar.',
    difficulty: 2,
    related_concept_id: 'llm-observability',
  },

  // --- embracing-risk ---
  {
    concept_id: 'embracing-risk',
    type: 'definition',
    question_text: '¿Por qué Google SRE dice que 100% reliability es un objetivo incorrecto?',
    expected_answer: 'Porque 100% reliability es infinitamente costoso e imposible de lograr. Además, los usuarios no notan la diferencia entre 99.99% y 100% dado que hay otros factores de falla (su red, su dispositivo). Mejor definir error budgets y gastarlos en velocidad de innovación.',
    difficulty: 1,
  },
  {
    concept_id: 'embracing-risk',
    type: 'fact',
    question_text: '¿Cómo se calcula el costo incremental de cada "nueve" de reliability?',
    expected_answer: 'Cada nueve adicional es ~10x más costoso. Ir de 99% a 99.9% puede requerir redundancia. De 99.9% a 99.99% requiere failover automático, testing extensivo. De 99.99% a 99.999% requiere arquitecturas multi-region y eliminación de single points of failure.',
    difficulty: 2,
  },

  // --- tail-latency ---
  {
    concept_id: 'tail-latency',
    type: 'definition',
    question_text: '¿Por qué la tail latency (p99) es más importante que la latencia promedio?',
    expected_answer: 'Porque en sistemas distribuidos con fan-out, la latencia total es determinada por el componente más lento. Si haces 100 llamadas paralelas, la probabilidad de que al menos una sea p99 es alta. Los usuarios con más datos suelen tener peor latencia y son los más valiosos.',
    difficulty: 1,
  },
  {
    concept_id: 'tail-latency',
    type: 'property',
    question_text: '¿Qué son hedged requests y cómo reducen la tail latency?',
    expected_answer: 'Enviar el mismo request a múltiples réplicas y usar la primera respuesta. Reduce tail latency porque es improbable que todas las réplicas sean lentas simultáneamente. Trade-off: genera carga extra, por eso se usa solo cuando la primera respuesta tarda más de un umbral.',
    difficulty: 2,
  },

  // ==========================================================================
  // PHASE 2: LLM Fundamentals + Reasoning/Agents
  // ==========================================================================

  // --- attention-mechanism ---
  {
    concept_id: 'attention-mechanism',
    type: 'definition',
    question_text: '¿Qué es el mecanismo de atención y qué problema resuelve?',
    expected_answer: 'Es un mecanismo que permite al modelo enfocarse en partes relevantes del input al producir cada parte del output. Resuelve el bottleneck de las RNNs donde toda la información del input se comprime en un solo vector de tamaño fijo.',
    difficulty: 1,
  },
  {
    concept_id: 'attention-mechanism',
    type: 'property',
    question_text: '¿Qué es self-attention y en qué se diferencia de cross-attention?',
    expected_answer: 'Self-attention: cada posición atiende a todas las posiciones en la misma secuencia (input atiende a input). Cross-attention: las queries vienen de una secuencia y los keys/values de otra (decoder atiende a encoder). Self-attention es el mecanismo central del Transformer.',
    difficulty: 2,
  },
  {
    concept_id: 'attention-mechanism',
    type: 'complexity',
    question_text: '¿Cuál es la complejidad computacional de self-attention y por qué es un problema?',
    expected_answer: 'O(n²) en la longitud de la secuencia, tanto en tiempo como en memoria. Para una secuencia de 100K tokens, son 10 billones de operaciones por capa. Esto limita el context window y es la razón de técnicas como sparse attention, flash attention, y sliding window attention.',
    difficulty: 3,
  },

  // --- transformer-architecture ---
  {
    concept_id: 'transformer-architecture',
    type: 'definition',
    question_text: '¿Cuáles son los componentes principales de un Transformer?',
    expected_answer: 'Multi-head attention (múltiples mecanismos de atención en paralelo), positional encoding (información de posición), feed-forward layers (transformación no-lineal por posición), layer normalization, y residual connections.',
    difficulty: 1,
  },
  {
    concept_id: 'transformer-architecture',
    type: 'fact',
    question_text: '¿Por qué el Transformer eliminó la recurrencia y qué ventaja da?',
    expected_answer: 'Las RNNs procesan tokens secuencialmente, impidiendo paralelización. El Transformer usa atención para capturar dependencias sin recurrencia, permitiendo procesar todos los tokens en paralelo durante el entrenamiento. Esto redujo dramáticamente el tiempo de entrenamiento.',
    difficulty: 2,
  },
  {
    concept_id: 'transformer-architecture',
    type: 'property',
    question_text: '¿Qué son las residual connections y por qué son necesarias en Transformers profundos?',
    expected_answer: 'Son conexiones que suman el input de una capa directamente a su output (x + f(x)). Permiten que los gradientes fluyan directamente a través de la red, evitando el vanishing gradient problem en redes profundas. Sin ellas, entrenar Transformers con muchas capas sería muy difícil.',
    difficulty: 2,
  },

  // --- query-key-value ---
  {
    concept_id: 'query-key-value',
    type: 'definition',
    question_text: '¿Qué representan Query, Key y Value en attention y cuál es la fórmula?',
    expected_answer: 'Query: qué estoy buscando. Key: qué contengo (para matching). Value: qué proveo si me seleccionan. Fórmula: Attention(Q,K,V) = softmax(QK^T / √d_k) V. La división por √d_k estabiliza los gradientes.',
    difficulty: 1,
  },
  {
    concept_id: 'query-key-value',
    type: 'fact',
    question_text: '¿Por qué se divide por √d_k en la fórmula de attention?',
    expected_answer: 'Sin la división, los productos punto QK^T crecen con la dimensión d_k, produciendo valores muy grandes. El softmax de valores grandes resulta en gradientes extremadamente pequeños (saturación). Dividir por √d_k mantiene la varianza del producto punto en ~1.',
    difficulty: 2,
  },
  {
    concept_id: 'query-key-value',
    type: 'property',
    question_text: '¿Qué es multi-head attention y por qué es mejor que single-head?',
    expected_answer: 'En vez de una sola función de atención, se proyectan Q, K, V con diferentes matrices aprendidas h veces, se computa atención en paralelo, y se concatenan los resultados. Permite al modelo atender a información de diferentes subspacios simultáneamente (ej: una head para sintaxis, otra para semántica).',
    difficulty: 2,
  },

  // --- positional-encoding ---
  {
    concept_id: 'positional-encoding',
    type: 'definition',
    question_text: '¿Por qué los Transformers necesitan positional encoding?',
    expected_answer: 'Porque self-attention es permutation-invariant: trata el input como un conjunto, no como una secuencia. Sin positional encoding, "el gato come pescado" y "pescado come el gato" producirían la misma representación.',
    difficulty: 1,
  },
  {
    concept_id: 'positional-encoding',
    type: 'comparison',
    question_text: '¿Cuáles son los tipos principales de positional encoding?',
    expected_answer: 'Sinusoidal (original Transformer): funciones seno/coseno de diferentes frecuencias, no aprendido. Learned: embeddings de posición entrenados. RoPE (Rotary Position Embeddings): codifica posición rotando los vectores Q y K, permite extrapolación a secuencias más largas que las vistas en training.',
    difficulty: 2,
    related_concept_id: 'transformer-architecture',
  },

  // --- scaling-laws ---
  {
    concept_id: 'scaling-laws',
    type: 'definition',
    question_text: '¿Qué son las scaling laws de LLMs?',
    expected_answer: 'Son relaciones empíricas que muestran que el loss de un LLM escala como ley de potencia con el tamaño del modelo (parámetros), la cantidad de datos de entrenamiento, y el compute total. Permiten predecir el rendimiento antes de entrenar.',
    difficulty: 1,
  },
  {
    concept_id: 'scaling-laws',
    type: 'fact',
    question_text: '¿Cuáles son las implicaciones prácticas de las scaling laws para el entrenamiento?',
    expected_answer: 'Ayudan a decidir cómo distribuir un presupuesto fijo de compute entre tamaño de modelo y datos. También muestran que no hay "techo" observable: más compute siempre mejora performance. Esto justifica las inversiones masivas en training runs.',
    difficulty: 2,
  },

  // --- compute-optimal-training ---
  {
    concept_id: 'compute-optimal-training',
    type: 'definition',
    question_text: '¿Cuál fue el hallazgo clave del paper Chinchilla sobre entrenamiento óptimo?',
    expected_answer: 'Que la mayoría de modelos grandes estaban significativamente sub-entrenados (pocos datos para su tamaño). Lo óptimo es escalar datos y parámetros igualmente. Chinchilla (70B params, 1.4T tokens) superó a Gopher (280B params, 300B tokens) con 4x menos parámetros.',
    difficulty: 1,
  },
  {
    concept_id: 'compute-optimal-training',
    type: 'fact',
    question_text: '¿Cuál es la ratio óptima de tokens a parámetros según Chinchilla?',
    expected_answer: 'Aproximadamente 20 tokens por parámetro. Un modelo de 10B necesita ~200B tokens de training data. Modelos como LLaMA siguieron esta receta: 7B con 1T tokens, 65B con 1.4T tokens.',
    difficulty: 2,
  },

  // --- foundation-models ---
  {
    concept_id: 'foundation-models',
    type: 'definition',
    question_text: '¿Qué son los foundation models y qué los hace diferentes?',
    expected_answer: 'Son modelos grandes entrenados con datos amplios que pueden adaptarse a muchas tareas downstream. A diferencia de modelos task-specific, una sola preentrenamiento sirve para múltiples aplicaciones. Exhiben habilidades emergentes que aparecen solo a cierta escala.',
    difficulty: 1,
  },
  {
    concept_id: 'foundation-models',
    type: 'property',
    question_text: '¿Qué son las emergent abilities en foundation models?',
    expected_answer: 'Son capacidades que no están presentes en modelos pequeños pero emergen a cierta escala: few-shot learning, chain-of-thought reasoning, arithmetic. Son difíciles de predecir con las scaling laws porque aparecen abruptamente, no gradualmente.',
    difficulty: 2,
  },

  // --- react-pattern ---
  {
    concept_id: 'react-pattern',
    type: 'definition',
    question_text: '¿Qué es el patrón ReAct y cómo combina reasoning y acting?',
    expected_answer: 'ReAct intercala razonamiento (chain-of-thought) con acciones (tool use). El modelo piensa paso a paso, toma una acción (buscar, calcular), observa el resultado, y repite. Produce reasoning grounded en información real y trazas auditables.',
    difficulty: 1,
  },
  {
    concept_id: 'react-pattern',
    type: 'property',
    question_text: '¿Cuáles son las tres fases del loop ReAct?',
    expected_answer: 'Thought (razonamiento sobre qué hacer), Action (ejecutar una herramienta o acción), Observation (observar el resultado). El ciclo se repite hasta que el modelo decide que tiene suficiente información para responder.',
    difficulty: 1,
  },
  {
    concept_id: 'react-pattern',
    type: 'comparison',
    question_text: '¿En qué se diferencia ReAct de chain-of-thought puro?',
    expected_answer: 'Chain-of-thought solo razona internamente sin acceso a información externa. ReAct puede actuar (usar tools) y observar resultados reales, lo que evita confabular hechos. ReAct es más preciso para tareas que requieren información factual actualizada.',
    difficulty: 2,
    related_concept_id: 'chain-of-thought',
  },

  // --- chain-of-thought ---
  {
    concept_id: 'chain-of-thought',
    type: 'definition',
    question_text: '¿Qué es chain-of-thought prompting y por qué mejora el rendimiento?',
    expected_answer: 'Es hacer que el modelo muestre pasos de razonamiento intermedios antes de dar la respuesta final. Mejora performance en tareas complejas porque descompone el problema en pasos más simples. Puede ser few-shot (con ejemplos) o zero-shot ("think step by step").',
    difficulty: 1,
  },
  {
    concept_id: 'chain-of-thought',
    type: 'fact',
    question_text: '¿Qué es zero-shot CoT y cuál fue el hallazgo sorprendente?',
    expected_answer: 'Agregar simplemente "Let\'s think step by step" al prompt activa razonamiento en cadena sin necesitar ejemplos. Esto mostró que la capacidad de razonamiento ya existía en los modelos y solo necesitaba ser elicitada con el prompt correcto.',
    difficulty: 1,
  },

  // --- tree-of-thoughts ---
  {
    concept_id: 'tree-of-thoughts',
    type: 'definition',
    question_text: '¿Cómo extiende Tree of Thoughts al chain-of-thought estándar?',
    expected_answer: 'En vez de una sola cadena de razonamiento, explora múltiples caminos en forma de árbol. Puede evaluar cada rama (auto-evaluación), hacer backtracking (descartar malas ramas), y explorar breadth-first o depth-first. Permite resolución deliberada de problemas.',
    difficulty: 2,
  },
  {
    concept_id: 'tree-of-thoughts',
    type: 'property',
    question_text: '¿Qué tipos de problemas se benefician de Tree of Thoughts vs CoT lineal?',
    expected_answer: 'Problemas que requieren exploración (juegos, puzzles, planning), donde un error temprano invalida toda la cadena. CoT lineal es suficiente para problemas donde el razonamiento es más directo (matemáticas, lógica simple). ToT es más costoso (múltiples generaciones).',
    difficulty: 2,
  },

  // --- reflexion ---
  {
    concept_id: 'reflexion',
    type: 'definition',
    question_text: '¿Qué es Reflexion y cómo permite a los agentes aprender de errores?',
    expected_answer: 'Reflexion es un patrón donde el agente, tras fallar, genera una reflexión verbal sobre qué salió mal y la almacena en memoria. En intentos futuros, consulta estas reflexiones para evitar los mismos errores. Aprendizaje sin actualizar pesos del modelo.',
    difficulty: 2,
  },
  {
    concept_id: 'reflexion',
    type: 'property',
    question_text: '¿Qué componentes necesita un sistema Reflexion?',
    expected_answer: 'Un actor (genera acciones), un evaluador (determina éxito/fracaso), un modelo de reflexión (genera feedback verbal), y una memoria episódica (almacena las reflexiones). El actor consulta la memoria antes de cada intento.',
    difficulty: 2,
  },

  // --- tool-use ---
  {
    concept_id: 'tool-use',
    type: 'definition',
    question_text: '¿Qué es tool use en LLMs y por qué es importante?',
    expected_answer: 'Es la capacidad de LLMs de llamar APIs externas, calculadoras, buscadores, etc. Extiende las capacidades más allá de los datos de entrenamiento: acceso a información actualizada, cálculos precisos, acciones en el mundo real. El desafío es saber cuándo usar qué herramienta.',
    difficulty: 1,
  },
  {
    concept_id: 'tool-use',
    type: 'property',
    question_text: '¿Cómo se implementa function calling en la práctica?',
    expected_answer: 'Se define un esquema de funciones disponibles (nombre, descripción, parámetros) en el prompt o via API especial. El modelo genera una llamada a función (nombre + argumentos) en vez de texto. El sistema ejecuta la función y retorna el resultado al modelo para que continúe.',
    difficulty: 2,
  },

  // --- plan-and-execute ---
  {
    concept_id: 'plan-and-execute',
    type: 'definition',
    question_text: '¿Cómo funciona el patrón Plan-and-Execute y en qué se diferencia de ReAct?',
    expected_answer: 'Plan-and-Execute primero genera un plan completo (lista de pasos), luego ejecuta cada paso. ReAct intercala razonamiento y acción en cada paso. Plan-and-Execute es mejor para tareas complejas que requieren coordinación, y permite re-planificar si un paso falla.',
    difficulty: 2,
  },
  {
    concept_id: 'plan-and-execute',
    type: 'property',
    question_text: '¿Cuándo conviene re-planificar vs continuar con el plan original?',
    expected_answer: 'Re-planificar cuando un paso falla, el resultado es inesperado, o surge nueva información que invalida el plan. Continuar cuando los resultados están dentro de lo esperado. El re-planning tiene costo (tokens, latencia), así que se hace solo cuando es necesario.',
    difficulty: 2,
  },

  // --- prompt-engineering ---
  {
    concept_id: 'prompt-engineering',
    type: 'definition',
    question_text: '¿Qué técnicas principales componen el prompt engineering?',
    expected_answer: 'Few-shot learning (ejemplos en el prompt), role prompting (asignar un rol al modelo), format specification (definir formato de output), constraint setting (reglas y límites), chain-of-thought, y system vs user prompts para separar instrucciones de datos.',
    difficulty: 1,
  },
  {
    concept_id: 'prompt-engineering',
    type: 'fact',
    question_text: '¿Por qué el orden y la estructura del prompt importan?',
    expected_answer: 'Los LLMs tienen primacy bias (prestan más atención al inicio) y recency bias (y al final). Instrucciones importantes deben ir al inicio o al final. Delimitadores claros separan secciones. La estructura consistente (JSON, markdown) mejora la adherencia al formato.',
    difficulty: 2,
  },

  // --- structured-output ---
  {
    concept_id: 'structured-output',
    type: 'definition',
    question_text: '¿Qué técnicas existen para obtener structured output de LLMs?',
    expected_answer: 'JSON mode (API-level), function calling schemas, grammar-constrained decoding (restringir tokens válidos), format instructions en prompt, y parseo + validación post-generación con retry. Las más confiables son las API-level porque restringen la generación directamente.',
    difficulty: 1,
  },
  {
    concept_id: 'structured-output',
    type: 'property',
    question_text: '¿Qué es grammar-constrained decoding?',
    expected_answer: 'Restringir los tokens que el modelo puede generar en cada paso para que el output sea siempre un JSON/formato válido. Se usa una gramática formal (como GBNF) que define la estructura. Garantiza output válido pero puede afectar la calidad del contenido.',
    difficulty: 2,
  },

  // --- fine-tuning-efficiency ---
  {
    concept_id: 'fine-tuning-efficiency',
    type: 'definition',
    question_text: '¿Qué es LoRA y por qué es eficiente?',
    expected_answer: 'LoRA (Low-Rank Adaptation) congela los pesos del modelo y agrega matrices de bajo rango entrenables a cada capa. Solo entrena ~0.1-1% de los parámetros totales. Reduce compute y memoria 10-1000x manteniendo calidad comparable al fine-tuning completo.',
    difficulty: 1,
  },
  {
    concept_id: 'fine-tuning-efficiency',
    type: 'fact',
    question_text: '¿Cuál es la intuición matemática detrás de LoRA?',
    expected_answer: 'Los cambios necesarios en los pesos durante fine-tuning tienen bajo rango intrínseco. En vez de actualizar la matriz completa W (d×d), se aprenden dos matrices A (d×r) y B (r×d) donde r << d. W_nuevo = W + BA. Esto captura las adaptaciones necesarias con muchos menos parámetros.',
    difficulty: 3,
  },

  // --- dpo ---
  {
    concept_id: 'dpo',
    type: 'definition',
    question_text: '¿Qué es DPO y cómo se compara con RLHF?',
    expected_answer: 'DPO (Direct Preference Optimization) alinea modelos directamente desde pares de preferencias (buena respuesta vs mala) sin necesitar un reward model ni RL. Misma función objetivo que RLHF pero con un loss supervisado. Más simple, estable y eficiente de entrenar.',
    difficulty: 1,
  },
  {
    concept_id: 'dpo',
    type: 'property',
    question_text: '¿Por qué DPO es más estable que RLHF?',
    expected_answer: 'RLHF requiere entrenar un reward model (que puede ser impreciso), luego hacer RL (que es inestable). DPO elimina ambos pasos: convierte el problema de RL en clasificación binaria (cuál respuesta es mejor). Menos hiperparámetros, convergencia más predecible.',
    difficulty: 2,
  },

  // --- test-time-compute ---
  {
    concept_id: 'test-time-compute',
    type: 'definition',
    question_text: '¿Qué es test-time compute scaling y qué dimensión nueva abre?',
    expected_answer: 'Es usar más compute en inferencia (no en training) para mejorar el razonamiento. Modelos como o1 y DeepSeek-R1 "piensan más" generando cadenas de razonamiento largas antes de responder. Abre una nueva dimensión de scaling: además de model size y training data, también inference compute.',
    difficulty: 1,
  },
  {
    concept_id: 'test-time-compute',
    type: 'property',
    question_text: '¿Cuál es el trade-off fundamental del test-time compute?',
    expected_answer: 'Latencia por calidad: respuestas más precisas requieren más tiempo y tokens de generación. Más costoso por query. No todos los problemas se benefician: preguntas simples no necesitan razonamiento extenso. Se necesita routing inteligente para decidir cuánto compute usar.',
    difficulty: 2,
  },

  // --- reasoning-models ---
  {
    concept_id: 'reasoning-models',
    type: 'definition',
    question_text: '¿Qué son los reasoning models (o1, R1) y cómo se entrenan?',
    expected_answer: 'Son LLMs entrenados con RL para producir cadenas de razonamiento extensas antes de responder. No se les da datos de razonamiento etiquetados: la capacidad emerge del RL que recompensa respuestas correctas. Excelen en math, código, y razonamiento complejo.',
    difficulty: 1,
  },
  {
    concept_id: 'reasoning-models',
    type: 'fact',
    question_text: '¿Cuál fue el insight clave de DeepSeek-R1 respecto al entrenamiento?',
    expected_answer: 'Que la capacidad de razonamiento puede emerger puramente de RL sin supervisión de razonamiento. Solo necesitas recompensar la respuesta correcta (outcome-based RL) y el modelo aprende a generar cadenas de razonamiento como estrategia. El razonamiento emergió, no se programó.',
    difficulty: 2,
  },

  // --- multimodal-embeddings ---
  {
    concept_id: 'multimodal-embeddings',
    type: 'definition',
    question_text: '¿Qué es CLIP y cómo crea embeddings multimodales?',
    expected_answer: 'CLIP (Contrastive Language-Image Pre-training) entrena un image encoder y un text encoder juntos para que imágenes y textos similares tengan representaciones cercanas en el mismo espacio vectorial. Usa contrastive learning: acerca pares correctos y aleja pares incorrectos.',
    difficulty: 1,
  },
  {
    concept_id: 'multimodal-embeddings',
    type: 'property',
    question_text: '¿Por qué CLIP permite zero-shot image classification?',
    expected_answer: 'Porque comparte espacio de embeddings entre texto e imágenes. Para clasificar, embeds los nombres de las clases como texto y la imagen, luego compara similitud coseno. No necesita entrenar un clasificador específico: cualquier texto sirve como "clase".',
    difficulty: 2,
  },

  // --- vision-language-models ---
  {
    concept_id: 'vision-language-models',
    type: 'definition',
    question_text: '¿Cuál es la arquitectura típica de un VLM?',
    expected_answer: 'Vision encoder (como CLIP o ViT) que procesa la imagen, una capa de proyección que mapea features visuales al espacio del LLM, y un LLM que procesa tanto los tokens visuales proyectados como texto. Se entrena con visual instruction tuning.',
    difficulty: 1,
  },
  {
    concept_id: 'vision-language-models',
    type: 'fact',
    question_text: '¿Qué es visual instruction tuning y por qué fue un breakthrough?',
    expected_answer: 'Es fine-tuning de un VLM con datos de instrucciones multimodales (pregunta sobre imagen → respuesta). LLaVA demostró que con datos de instruction tuning generados por GPT-4, un modelo relativamente pequeño podía competir con GPT-4V. Hizo VLMs accesibles y replicables.',
    difficulty: 2,
  },

  // ==========================================================================
  // PHASE 3: RAG, Memory, Context
  // ==========================================================================

  // --- rag-basics ---
  {
    concept_id: 'rag-basics',
    type: 'definition',
    question_text: '¿Qué es RAG y qué tres componentes tiene?',
    expected_answer: 'RAG (Retrieval-Augmented Generation) aumenta la generación del LLM con documentos recuperados. Tres componentes: Retriever (busca documentos relevantes), Reader/Ranker (selecciona los mejores), y Generator (LLM que genera la respuesta usando el contexto recuperado).',
    difficulty: 1,
  },
  {
    concept_id: 'rag-basics',
    type: 'property',
    question_text: '¿Qué ventajas tiene RAG sobre fine-tuning para incorporar conocimiento?',
    expected_answer: 'Actualizable sin reentrenamiento (solo actualiza la base de datos), fuentes auditables (puedes citar de dónde viene la info), más barato que fine-tuning, y reduce hallucination al grounding en documentos reales. Fine-tuning es mejor para cambiar el estilo o formato del modelo.',
    difficulty: 2,
  },
  {
    concept_id: 'rag-basics',
    type: 'fact',
    question_text: '¿Cuáles son las métricas principales para evaluar un sistema RAG?',
    expected_answer: 'Para retrieval: precision (% de docs relevantes entre los recuperados) y recall (% de docs relevantes encontrados). Para generation: faithfulness (respuesta soportada por docs), relevance (respuesta contesta la pregunta), y answer correctness.',
    difficulty: 2,
  },

  // --- embeddings ---
  {
    concept_id: 'embeddings',
    type: 'definition',
    question_text: '¿Qué son embeddings y por qué permiten búsqueda semántica?',
    expected_answer: 'Son representaciones vectoriales densas de texto donde significados similares producen vectores cercanos en el espacio. Permiten búsqueda semántica porque "perro" y "canino" tendrán vectores cercanos aunque las palabras sean diferentes.',
    difficulty: 1,
  },
  {
    concept_id: 'embeddings',
    type: 'property',
    question_text: '¿Qué dimensiones y métricas de distancia se usan comúnmente en embeddings?',
    expected_answer: 'Dimensiones típicas: 768 (sentence-transformers), 1536 (OpenAI ada-002), 3072 (OpenAI large). Métricas de distancia: cosine similarity (la más común, normalizada), dot product (incluye magnitud), y euclidean distance. Cosine es preferida porque es invariante a la magnitud del vector.',
    difficulty: 2,
  },

  // --- vector-search ---
  {
    concept_id: 'vector-search',
    type: 'definition',
    question_text: '¿Qué es vector search y cuál es el trade-off entre exacto y aproximado?',
    expected_answer: 'Es buscar los vectores más similares en una base de datos de embeddings. Búsqueda exacta: O(n) brute force, perfecta pero lenta. Búsqueda aproximada (ANN): usa índices como HNSW o IVF, mucho más rápida pero puede perder algunos resultados. Trade-off: velocidad vs recall.',
    difficulty: 1,
  },
  {
    concept_id: 'vector-search',
    type: 'fact',
    question_text: '¿Cómo funciona HNSW y por qué es el algoritmo más popular?',
    expected_answer: 'HNSW (Hierarchical Navigable Small World) construye un grafo multi-nivel donde cada nivel es más disperso. La búsqueda empieza en el nivel más alto (pocos nodos, saltos grandes) y baja a niveles más densos para refinamiento. Popular por ser rápido, preciso, y no requerir entrenamiento.',
    difficulty: 2,
  },

  // --- chunking-strategies ---
  {
    concept_id: 'chunking-strategies',
    type: 'definition',
    question_text: '¿Por qué es necesario chunking en RAG y cuáles son las estrategias principales?',
    expected_answer: 'Los documentos son demasiado largos para embeddings y context windows. Estrategias: fixed size (ej: 512 tokens), por oraciones/párrafos, recursive (divide jerárquicamente), y semantic (agrupa por tema). Cada una tiene trade-offs entre granularidad y contexto.',
    difficulty: 1,
  },
  {
    concept_id: 'chunking-strategies',
    type: 'property',
    question_text: '¿Cuál es el trade-off entre chunks grandes y pequeños?',
    expected_answer: 'Chunks pequeños: embeddings más precisos (un tema por chunk), mejor recall, pero pierden contexto circundante. Chunks grandes: mantienen contexto pero el embedding mezcla múltiples temas, peor precision. Solución: parent-child chunking (embed el hijo, recuperar el padre).',
    difficulty: 2,
  },

  // --- lost-in-middle ---
  {
    concept_id: 'lost-in-middle',
    type: 'definition',
    question_text: '¿Qué descubrió el paper "Lost in the Middle" sobre LLMs y contextos largos?',
    expected_answer: 'Que los LLMs prestan menos atención a la información en el medio del contexto. El rendimiento tiene forma de U: mejor cuando la info relevante está al inicio o al final. Esto afecta cómo se deben ordenar los documentos en RAG.',
    difficulty: 1,
  },
  {
    concept_id: 'lost-in-middle',
    type: 'property',
    question_text: '¿Cómo se mitiga el efecto "lost in the middle" en sistemas RAG?',
    expected_answer: 'Poner documentos más relevantes al inicio y al final del contexto. Reducir la cantidad de documentos (solo los más relevantes). Usar técnicas de compresión de contexto. O simplemente mantener contextos cortos y bien filtrados.',
    difficulty: 2,
  },

  // --- context-window-limits ---
  {
    concept_id: 'context-window-limits',
    type: 'definition',
    question_text: '¿Qué limita el context window y por qué más contexto no siempre es mejor?',
    expected_answer: 'El context window está limitado por la memoria (atención es O(n²)) y el costo (tokens). Más contexto no es mejor porque: el modelo puede distraerse con info irrelevante, lost-in-middle, y el costo escala linealmente con el tamaño.',
    difficulty: 1,
  },
  {
    concept_id: 'context-window-limits',
    type: 'fact',
    question_text: '¿Cuál es la diferencia entre "long context" del modelo y la calidad de uso de ese contexto?',
    expected_answer: 'Un modelo puede aceptar 128K tokens pero eso no significa que los use efectivamente. Needle-in-haystack tests muestran degradación en contextos largos. La capacidad real de procesamiento suele ser menor que el máximo técnico. Más importante que el tamaño es cuán bien se usa.',
    difficulty: 2,
  },

  // --- external-memory ---
  {
    concept_id: 'external-memory',
    type: 'definition',
    question_text: '¿Qué es external memory para LLMs y qué problema resuelve?',
    expected_answer: 'Es almacenar información fuera del contexto del modelo (en vector DBs, key-value stores) y recuperarla cuando se necesita. Resuelve la limitación del context window: permite acceso a conocimiento ilimitado sin los costos y limitaciones de contextos largos.',
    difficulty: 1,
  },
  {
    concept_id: 'external-memory',
    type: 'comparison',
    question_text: '¿Cuál es la diferencia entre external memory y RAG?',
    expected_answer: 'RAG recupera documentos para responder una pregunta específica. External memory es más amplio: incluye memorias episódicas (conversaciones pasadas), conocimiento persistente, y estado del usuario. MemGPT por ejemplo usa un sistema de paginación de memoria inspirado en OS.',
    difficulty: 2,
    related_concept_id: 'rag-basics',
  },

  // --- hybrid-search ---
  {
    concept_id: 'hybrid-search',
    type: 'definition',
    question_text: '¿Qué es hybrid search y por qué supera a búsqueda semántica sola?',
    expected_answer: 'Combina búsqueda densa (embeddings/semántica) con búsqueda sparse (keywords/BM25). La semántica encuentra sinónimos y paráfrasis. Keywords encuentra exactamente lo que pides (nombres propios, IDs, siglas). Juntas cubren más casos que cualquiera sola.',
    difficulty: 1,
  },
  {
    concept_id: 'hybrid-search',
    type: 'property',
    question_text: '¿Qué es Reciprocal Rank Fusion (RRF) y para qué se usa?',
    expected_answer: 'RRF es una técnica para combinar rankings de diferentes fuentes. Asigna un score basado en la posición en cada ranking: 1/(k + rank). Luego suma los scores. Es simple, no necesita normalización, y funciona bien en la práctica para fusionar resultados de búsqueda dense y sparse.',
    difficulty: 2,
  },

  // --- memory-management ---
  {
    concept_id: 'memory-management',
    type: 'definition',
    question_text: '¿Por qué "guardar todo para siempre" no funciona como estrategia de memoria para LLMs?',
    expected_answer: 'Porque genera ruido: memorias obsoletas o irrelevantes contaminan las recuperaciones. Además, los costos de storage y búsqueda crecen sin control. Se necesitan políticas activas de olvido, summarización, y priorización por importancia y recencia.',
    difficulty: 1,
  },
  {
    concept_id: 'memory-management',
    type: 'property',
    question_text: '¿Qué políticas de memoria se usan para LLM agents?',
    expected_answer: 'Recency bias (priorizar memorias recientes), importance scoring (memorias sobre eventos significativos pesan más), summarización (condensar memorias detalladas en resúmenes), conflict resolution (cuándo dos memorias se contradicen), y TTL/expiration para datos temporales.',
    difficulty: 2,
  },

  // ==========================================================================
  // PHASE 4: Safety, Guardrails, Evaluation
  // ==========================================================================

  // --- constitutional-ai ---
  {
    concept_id: 'constitutional-ai',
    type: 'definition',
    question_text: '¿Qué es Constitutional AI y cómo funciona?',
    expected_answer: 'Es un método de Anthropic para alinear modelos usando principios escritos (una "constitución"). El modelo genera respuestas, luego se auto-critica y revisa según los principios. Más escalable que RLHF con humanos porque la constitución se puede actualizar sin re-anotar datos.',
    difficulty: 1,
  },
  {
    concept_id: 'constitutional-ai',
    type: 'property',
    question_text: '¿Cuáles son las dos fases de Constitutional AI?',
    expected_answer: 'Fase 1 (Supervised): el modelo genera, se auto-critica según la constitución, y se revisa. Los pares revisados se usan para fine-tuning. Fase 2 (RL): se entrena un reward model con preferencias generadas por el propio modelo (RLAIF), luego RL contra ese reward model.',
    difficulty: 2,
  },

  // --- self-consistency ---
  {
    concept_id: 'self-consistency',
    type: 'definition',
    question_text: '¿Qué es self-consistency y cómo mejora la fiabilidad?',
    expected_answer: 'Es generar múltiples cadenas de razonamiento para la misma pregunta y tomar el voto mayoritario. Si 4 de 5 cadenas llegan a la misma respuesta, es más probable que sea correcta. Mejora accuracy en tareas de razonamiento a costa de generar múltiples respuestas.',
    difficulty: 1,
  },
  {
    concept_id: 'self-consistency',
    type: 'property',
    question_text: '¿Cuál es el trade-off principal de self-consistency?',
    expected_answer: 'Costo: necesitas generar N respuestas (típicamente 5-40) para una pregunta. Si cada generación cuesta $X, self-consistency cuesta $NX. Funciona mejor con temperature > 0 para diversidad. No ayuda si el modelo falla consistentemente por falta de conocimiento.',
    difficulty: 2,
  },

  // --- offline-evaluation ---
  {
    concept_id: 'offline-evaluation',
    type: 'definition',
    question_text: '¿Qué es offline evaluation de LLMs y cuáles son sus limitaciones?',
    expected_answer: 'Es evaluar modelos con benchmarks fijos (MMLU, HumanEval, etc.) antes de producción. Limitaciones: benchmark contamination (datos en training), no captura comportamiento real del usuario, métricas estáticas vs dinámicas, y el LLM-as-judge tiene sus propios biases.',
    difficulty: 1,
  },
  {
    concept_id: 'offline-evaluation',
    type: 'property',
    question_text: '¿Qué es LLM-as-judge y cuándo es apropiado?',
    expected_answer: 'Usar un LLM (típicamente más capaz) para evaluar outputs de otro LLM. Apropiado para evaluar calidad de texto, coherencia, y adherencia a instrucciones. Menos apropiado para factuality (el juez puede no saber la verdad) y requiere calibración contra evaluación humana.',
    difficulty: 2,
  },

  // --- online-evaluation ---
  {
    concept_id: 'online-evaluation',
    type: 'definition',
    question_text: '¿Qué es online evaluation y por qué complementa a la offline?',
    expected_answer: 'Es evaluar LLMs en producción con usuarios reales: A/B tests, shadow deployments, canary releases. Captura lo que benchmarks no pueden: satisfacción real, edge cases imprevistos, y cómo el modelo interactúa con el sistema completo.',
    difficulty: 1,
  },
  {
    concept_id: 'online-evaluation',
    type: 'fact',
    question_text: '¿Qué métricas se monitorean en online evaluation de LLMs?',
    expected_answer: 'Latencia y throughput (performance), user satisfaction (thumbs up/down, retention), task completion rate, hallucination rate (feedback negativo), cost per query, y safety incidents. Combinar métricas cuantitativas con feedback cualitativo.',
    difficulty: 2,
  },

  // --- red-teaming ---
  {
    concept_id: 'red-teaming',
    type: 'definition',
    question_text: '¿Qué es red teaming para LLMs?',
    expected_answer: 'Es pruebas adversarias para encontrar fallos: jailbreaks (bypass de safety), outputs dañinos, bias, leakage de información privada. Se hace manual (expertos) y automatizado (LLMs atacando LLMs). Esencial antes de deployment.',
    difficulty: 1,
  },
  {
    concept_id: 'red-teaming',
    type: 'property',
    question_text: '¿Cuáles son las categorías principales de ataques en red teaming de LLMs?',
    expected_answer: 'Jailbreaks (evadir safety training), prompt injection (override de instrucciones), data extraction (obtener datos de training), bias elicitation (provocar respuestas sesgadas), capability elicitation (hacer que el modelo haga cosas que no debería), y consistency testing.',
    difficulty: 2,
  },

  // --- output-validation ---
  {
    concept_id: 'output-validation',
    type: 'definition',
    question_text: '¿Qué es output validation y qué capas incluye?',
    expected_answer: 'Verificar outputs del LLM antes de usarlos: schema validation (JSON válido), content validation (no hay alucinaciones), safety filtering (no contenido dañino), y format validation (cumple el formato requerido). Defense in depth contra generaciones poco confiables.',
    difficulty: 1,
  },
  {
    concept_id: 'output-validation',
    type: 'property',
    question_text: '¿Cuál es el patrón de retry con output validation?',
    expected_answer: 'Generar → Validar → Si falla, incluir el error en el prompt y regenerar → Validar de nuevo → Si falla N veces, fallback (respuesta genérica, escalación humana). Importante: limitar retries (costo) y tener fallback claro. Zod/Pydantic son comunes para schema validation.',
    difficulty: 2,
  },

  // --- prompt-injection ---
  {
    concept_id: 'prompt-injection',
    type: 'definition',
    question_text: '¿Qué es prompt injection y cuáles son los dos tipos principales?',
    expected_answer: 'Es un ataque donde input malicioso sobrescribe las instrucciones del sistema. Direct injection: el usuario incluye instrucciones en su input. Indirect injection: datos externos (docs RAG, tool outputs) contienen instrucciones maliciosas. Análogo a SQL injection.',
    difficulty: 1,
  },
  {
    concept_id: 'prompt-injection',
    type: 'property',
    question_text: '¿Qué defensas existen contra prompt injection?',
    expected_answer: 'Input sanitization (filtrar patrones conocidos), output filtering (validar que el output cumple la tarea), privilege separation (el LLM no tiene acceso directo a datos sensibles), instruction hierarchy (system > user), y detección con clasificadores entrenados.',
    difficulty: 2,
  },

  // --- llm-security-owasp ---
  {
    concept_id: 'llm-security-owasp',
    type: 'definition',
    question_text: '¿Cuáles son las top 3 vulnerabilidades del OWASP Top 10 para LLMs?',
    expected_answer: 'LLM01: Prompt Injection (direct e indirect). LLM02: Insecure Output Handling (confiar en outputs sin validar). LLM03: Training Data Poisoning (datos maliciosos en el training). Estas tres cubren los vectores de ataque más comunes.',
    difficulty: 1,
  },
  {
    concept_id: 'llm-security-owasp',
    type: 'property',
    question_text: '¿Qué es "excessive agency" en el contexto de seguridad LLM?',
    expected_answer: 'Cuando un LLM tiene acceso a funciones, datos, o permisos innecesarios para su tarea. Si el modelo es manipulado (prompt injection), puede abusar de esos permisos. Principio de least privilege: dar al LLM solo los tools y accesos mínimos necesarios.',
    difficulty: 2,
  },

  // ==========================================================================
  // PHASE 5: Inference, Serving, Economics
  // ==========================================================================

  // --- kv-cache ---
  {
    concept_id: 'kv-cache',
    type: 'definition',
    question_text: '¿Qué es el KV cache y por qué es esencial para inference eficiente?',
    expected_answer: 'El KV cache almacena los key-value pairs computados para tokens anteriores durante la generación. Sin él, cada token nuevo requeriría recalcular attention sobre todos los tokens previos. Con KV cache, solo se computa attention del nuevo token contra los K/V cacheados.',
    difficulty: 1,
  },
  {
    concept_id: 'kv-cache',
    type: 'property',
    question_text: '¿Cómo escala el uso de memoria del KV cache?',
    expected_answer: 'Escala como: batch_size × sequence_length × num_layers × 2 × hidden_dim × precision. Para un modelo de 70B con secuencia de 4K y batch 32, puede usar ~40GB de memoria. Esta es la principal limitación del batch size y sequence length en inference.',
    difficulty: 2,
  },

  // --- batching-inference ---
  {
    concept_id: 'batching-inference',
    type: 'definition',
    question_text: '¿Qué es continuous batching y por qué superó al static batching?',
    expected_answer: 'Static batching procesa un batch entero y espera a que todos terminen. Continuous batching agrega nuevos requests al batch tan pronto como otros terminan, a nivel de iteración. Mejora throughput significativamente porque no hay GPUs idle esperando al request más lento.',
    difficulty: 2,
  },
  {
    concept_id: 'batching-inference',
    type: 'property',
    question_text: '¿Cuál es el desafío principal del batching para LLM inference?',
    expected_answer: 'Los requests tienen longitudes de entrada y salida diferentes. Requests cortos terminan rápido y los largos demoran. Sin continuous batching, los cortos esperan a los largos. Además, el KV cache de cada request ocupa memoria diferente, complicando el memory management.',
    difficulty: 2,
  },

  // --- paged-attention ---
  {
    concept_id: 'paged-attention',
    type: 'definition',
    question_text: '¿Qué es PagedAttention y qué problema resuelve?',
    expected_answer: 'PagedAttention (de vLLM) almacena el KV cache en bloques no contiguos, como la memoria virtual de un OS. Resuelve la fragmentación de memoria: el KV cache ya no necesita ser contiguo. Permite memory sharing (ej: beam search) y mayor batch size.',
    difficulty: 2,
  },
  {
    concept_id: 'paged-attention',
    type: 'fact',
    question_text: '¿Cuánto mejora vLLM el throughput comparado con inference naive?',
    expected_answer: 'vLLM reporta 2-4x mejor throughput que HuggingFace Transformers y 2.2x sobre TGI en varios benchmarks. La mejora viene de mejor utilización de GPU memory (menos desperdicio) y continuous batching.',
    difficulty: 2,
  },

  // --- speculative-decoding ---
  {
    concept_id: 'speculative-decoding',
    type: 'definition',
    question_text: '¿Cómo funciona speculative decoding?',
    expected_answer: 'Un modelo pequeño (draft) genera K tokens candidatos rápidamente. El modelo grande los verifica todos en paralelo en un solo forward pass. Los tokens aceptados se mantienen. Acelera inference 2-3x sin pérdida de calidad porque el output final es idéntico al del modelo grande.',
    difficulty: 2,
  },
  {
    concept_id: 'speculative-decoding',
    type: 'property',
    question_text: '¿Por qué speculative decoding no pierde calidad?',
    expected_answer: 'Porque el modelo grande verifica cada token y rechaza los incorrectos. La distribución de probabilidad del output es matemáticamente idéntica a la del modelo grande generando solo. Solo se aceptan tokens que el modelo grande hubiera generado.',
    difficulty: 2,
  },

  // --- quantization ---
  {
    concept_id: 'quantization',
    type: 'definition',
    question_text: '¿Qué es quantization y cuáles son los niveles comunes?',
    expected_answer: 'Reducir la precisión de los pesos del modelo: FP32 → FP16 → INT8 → INT4. Reduce memoria (INT8 es 2x menos que FP16, INT4 es 4x menos) y acelera inference. Trade-off: calidad puede degradarse especialmente en INT4.',
    difficulty: 1,
  },
  {
    concept_id: 'quantization',
    type: 'fact',
    question_text: '¿Cuál es la diferencia entre post-training quantization (PTQ) y quantization-aware training (QAT)?',
    expected_answer: 'PTQ: cuantiza después de entrenar, más simple pero puede perder más calidad. QAT: simula cuantización durante training para que el modelo se adapte, mejor calidad pero requiere reentrenar. Para LLMs grandes, PTQ con calibración (GPTQ, AWQ) es lo más común.',
    difficulty: 2,
  },

  // --- token-economics ---
  {
    concept_id: 'token-economics',
    type: 'definition',
    question_text: '¿Cómo se estructura el pricing de LLM APIs y por qué importa?',
    expected_answer: 'Se cobra por millón de tokens, separando input y output. Output es 3-5x más caro que input porque requiere generación secuencial (más compute por token). Entender esto es clave para optimizar costos: reducir output tokens tiene más impacto que reducir input.',
    difficulty: 1,
  },
  {
    concept_id: 'token-economics',
    type: 'property',
    question_text: '¿Qué estrategias reducen costos de LLM APIs?',
    expected_answer: 'Prompt caching (reutilizar prefijos), model routing (queries simples a modelos baratos), semantic caching (evitar queries repetidas), batching (descuentos por volumen), reducir output length, y usar context compression para inputs largos.',
    difficulty: 2,
  },

  // --- model-routing ---
  {
    concept_id: 'model-routing',
    type: 'definition',
    question_text: '¿Qué es model routing y por qué no usar siempre el mejor modelo?',
    expected_answer: 'Es dirigir queries a diferentes modelos según complejidad, latencia requerida, y costo. No se usa siempre el mejor porque es caro y lento. Queries simples (ej: clasificación) pueden resolverse con modelos pequeños 10x más baratos con la misma calidad.',
    difficulty: 1,
  },
  {
    concept_id: 'model-routing',
    type: 'property',
    question_text: '¿Cómo se clasifica la complejidad de un query para routing?',
    expected_answer: 'Clasificador ligero (modelo pequeño o embeddings) que estima complejidad. Features: longitud del query, presencia de instrucciones complejas, dominio técnico. También se puede usar cascading: intentar con modelo barato primero, escalar al caro solo si la confianza es baja.',
    difficulty: 2,
  },

  // --- semantic-caching ---
  {
    concept_id: 'semantic-caching',
    type: 'definition',
    question_text: '¿Qué es semantic caching y cómo se diferencia del caching exacto?',
    expected_answer: 'Semantic caching usa embeddings para encontrar queries similares (no idénticas) y retornar respuestas cacheadas. A diferencia del caching exacto, "¿Qué es Python?" y "Explicame Python" pueden ser un cache hit. Usa un threshold de similitud coseno.',
    difficulty: 1,
  },
  {
    concept_id: 'semantic-caching',
    type: 'property',
    question_text: '¿Cuál es el riesgo principal del semantic caching?',
    expected_answer: 'Retornar respuestas incorrectas si el threshold es muy bajo: queries similares pero con diferentes intenciones ("Python programming" vs "Python snake"). Requiere tuning cuidadoso del threshold y posiblemente validación del contexto. Una respuesta cacheada incorrecta es peor que una generación lenta.',
    difficulty: 2,
  },

  // --- prompt-caching ---
  {
    concept_id: 'prompt-caching',
    type: 'definition',
    question_text: '¿Qué es prompt caching (prefix caching) y cómo difiere del semantic caching?',
    expected_answer: 'Prompt caching es una optimización API-level que cachea los KV states computados para prefijos de prompts. Si dos requests comparten el mismo system prompt, los tokens del prefijo se procesan una sola vez. A diferencia de semantic caching, requiere coincidencia exacta del prefijo.',
    difficulty: 1,
  },
  {
    concept_id: 'prompt-caching',
    type: 'fact',
    question_text: '¿Cuánto ahorro genera prompt caching y cómo se estructura el prompt para aprovecharlo?',
    expected_answer: 'Los tokens cacheados son 75-90% más baratos. Para aprovecharlo, se estructura: contenido estático primero (system prompt, instrucciones, context largo) y contenido variable al final (pregunta del usuario). El prefijo estático se cachea entre requests.',
    difficulty: 2,
  },

  // --- llm-observability ---
  {
    concept_id: 'llm-observability',
    type: 'definition',
    question_text: '¿En qué se diferencia LLM observability de la observability tradicional?',
    expected_answer: 'Además de métricas clásicas (latencia, errores), LLM observability requiere: tracking de prompts y completions, uso de tokens y costos por query, latency breakdown por componente (retrieval, generation), detección de hallucinations, y quality metrics a lo largo del tiempo.',
    difficulty: 1,
  },
  {
    concept_id: 'llm-observability',
    type: 'property',
    question_text: '¿Qué herramientas y patrones se usan para LLM observability?',
    expected_answer: 'Langfuse, LangSmith, Phoenix para tracing de LLM. Patrones: trace por request (incluye retrieval, prompts, completions), cost attribution (costo por feature/usuario), quality scoring automático, y alertas por degradación de métricas de calidad.',
    difficulty: 2,
  },

  // --- rate-limiting ---
  {
    concept_id: 'rate-limiting',
    type: 'definition',
    question_text: '¿Cuál es la diferencia entre rate limiting y backpressure?',
    expected_answer: 'Rate limiting es proactivo: establece un techo de requests (ej: 100 RPM) y rechaza los excedentes. Backpressure es reactivo: cuando un componente está sobrecargado, señala upstream para que reduzca la velocidad. Ambos previenen sobrecarga pero actúan en diferentes puntos.',
    difficulty: 1,
  },
  {
    concept_id: 'rate-limiting',
    type: 'property',
    question_text: '¿Por qué rate limiting es especialmente importante para LLM APIs?',
    expected_answer: 'Los LLM APIs tienen rate limits estrictos (tokens/min, requests/min). Un retry excesivo puede agotar el budget. Un burst de requests puede causar 429 errors en cascada. Se necesita: token bucket, exponential backoff, queue management, y priorización de requests.',
    difficulty: 2,
  },

  // --- compound-ai-systems ---
  {
    concept_id: 'compound-ai-systems',
    type: 'definition',
    question_text: '¿Qué son compound AI systems y por qué son el estado del arte?',
    expected_answer: 'Son sistemas de IA compuestos por múltiples componentes: modelos, retrievers, tools, guardrails, routers. Los resultados SOTA vienen del diseño del sistema, no solo de mejores modelos. 60% de apps LLM usan RAG, 30% usan multi-step chains.',
    difficulty: 1,
  },
  {
    concept_id: 'compound-ai-systems',
    type: 'property',
    question_text: '¿Qué desafíos de diseño introduce un compound AI system vs un solo modelo?',
    expected_answer: 'Optimización end-to-end (cada componente afecta al siguiente), debugging distribuido (¿dónde falló?), latency budgeting (cómo distribuir el tiempo entre componentes), error propagation (un componente malo afecta todo), y cost management (múltiples llamadas a LLM).',
    difficulty: 2,
  },

  // ==========================================================================
  // PHASE 6: Frameworks
  // ==========================================================================

  // --- langchain-architecture ---
  {
    concept_id: 'langchain-architecture',
    type: 'definition',
    question_text: '¿Cuáles son los componentes principales de LangChain?',
    expected_answer: 'Chains (secuencias de operaciones), Agents (LLM decide qué herramienta usar), Memory (contexto entre interacciones), Tools (funciones que el agente puede llamar), Retrievers (búsqueda en documentos), y Callbacks (observability).',
    difficulty: 1,
  },
  {
    concept_id: 'langchain-architecture',
    type: 'property',
    question_text: '¿Cuáles son las críticas principales a LangChain?',
    expected_answer: 'Abstraction overhead (muchas capas ocultan lo que pasa), debugging difícil (errores crípticos en chains profundas), performance costs (overhead innecesario), API inestable (breaking changes frecuentes), y "framework lock-in" que dificulta personalización.',
    difficulty: 2,
  },

  // --- llamaindex-architecture ---
  {
    concept_id: 'llamaindex-architecture',
    type: 'definition',
    question_text: '¿Qué diferencia a LlamaIndex de LangChain en enfoque?',
    expected_answer: 'LlamaIndex está enfocado en data ingestion y retrieval: conectores de datos, estrategias de indexación, query engines. LangChain es más general (chains, agents, tools). LlamaIndex es mejor para RAG-specific use cases. Ambos pueden usarse juntos.',
    difficulty: 1,
  },
  {
    concept_id: 'llamaindex-architecture',
    type: 'property',
    question_text: '¿Qué tipos de índices ofrece LlamaIndex?',
    expected_answer: 'Vector index (búsqueda semántica), Tree index (resumen jerárquico), List index (procesamiento secuencial), y Keyword Table index (búsqueda por keywords). Cada uno tiene trade-offs de costo, latencia, y calidad de retrieval.',
    difficulty: 2,
  },

  // --- framework-tradeoffs ---
  {
    concept_id: 'framework-tradeoffs',
    type: 'definition',
    question_text: '¿Cuándo conviene usar un framework vs construir desde cero?',
    expected_answer: 'Framework: prototipos rápidos, exploración, equipo sin experiencia LLM. Desde cero: producción con requisitos específicos, cuando necesitas control total (debugging, performance, personalización). Los mejores ingenieros entienden ambos caminos.',
    difficulty: 1,
  },
  {
    concept_id: 'framework-tradeoffs',
    type: 'comparison',
    question_text: '¿Qué se pierde al usar un framework en producción?',
    expected_answer: 'Control sobre el flujo de ejecución, visibilidad en errores internos, performance (overhead de abstracciones), y flexibilidad para optimizar (ej: prompt caching, batching específico). También dependency risk: updates del framework pueden romper tu código.',
    difficulty: 2,
    related_concept_id: 'minimal-implementations',
  },

  // --- minimal-implementations ---
  {
    concept_id: 'minimal-implementations',
    type: 'definition',
    question_text: '¿Qué demuestra poder construir RAG o un agent en ~200 líneas sin frameworks?',
    expected_answer: 'Que entiendes los mecanismos fundamentales: embedding + retrieval + prompt construction para RAG, tool calling + loop + parsing para agents. Sin este entendimiento, usar frameworks es "magia" que no puedes debuggear ni optimizar en producción.',
    difficulty: 1,
  },
  {
    concept_id: 'minimal-implementations',
    type: 'property',
    question_text: '¿Cuáles son los componentes mínimos de un RAG system desde cero?',
    expected_answer: 'Chunking (split documentos), embedding (vectorizar chunks), storage (guardar en vector DB), retrieval (buscar por similitud), y prompt construction (armar prompt con contexto + query). Con estas 5 piezas, tienes RAG funcional sin ningún framework.',
    difficulty: 2,
  },

  // --- dspy-programming ---
  {
    concept_id: 'dspy-programming',
    type: 'definition',
    question_text: '¿Qué es DSPy y en qué se diferencia del prompt engineering manual?',
    expected_answer: 'DSPy permite programar (no prompting) LLMs con módulos declarativos que se auto-optimizan. Defines la firma (input → output) y DSPy compila prompts efectivos automáticamente via optimización. Supera prompts manuales por 25-65% porque optimiza sistemáticamente.',
    difficulty: 2,
  },
  {
    concept_id: 'dspy-programming',
    type: 'property',
    question_text: '¿Qué es un DSPy "signature" y cómo funciona la compilación?',
    expected_answer: 'Un signature define input/output de un módulo (ej: "question -> answer"). La compilación usa un optimizador (ej: BootstrapFewShot) que genera y evalúa ejemplos, seleccionando los que mejor funcionan como few-shot demos. El resultado es un prompt optimizado para la tarea.',
    difficulty: 3,
  },
];

// ============================================================================
// SEED SCRIPT
// ============================================================================

async function seedQuestionBank() {
  console.log(`\n=== Seeding Question Bank ===\n`);
  console.log(`Total questions to insert: ${questions.length}`);

  // Verify concepts exist
  const conceptIds = [...new Set(questions.map(q => q.concept_id))];
  const { data: existingConcepts, error: conceptError } = await supabase
    .from('concepts')
    .select('id')
    .in('id', conceptIds);

  if (conceptError) {
    console.error('Error checking concepts:', conceptError);
    process.exit(1);
  }

  const existingIds = new Set((existingConcepts || []).map(c => c.id));
  const missingConcepts = conceptIds.filter(id => !existingIds.has(id));

  if (missingConcepts.length > 0) {
    console.error(`Missing concepts in database: ${missingConcepts.join(', ')}`);
    console.error('Run the seed-database.ts script first to populate concepts.');
    process.exit(1);
  }

  console.log(`All ${conceptIds.length} concepts verified.`);

  // Check for related_concept_id validity
  const relatedIds = questions
    .filter(q => q.related_concept_id)
    .map(q => q.related_concept_id!);
  const missingRelated = relatedIds.filter(id => !existingIds.has(id));
  if (missingRelated.length > 0) {
    console.warn(`Warning: related_concept_ids not in DB: ${missingRelated.join(', ')}`);
  }

  // Clear existing questions
  const { error: deleteError } = await supabase
    .from('question_bank')
    .delete()
    .neq('id', '00000000-0000-0000-0000-000000000000'); // delete all

  if (deleteError) {
    console.error('Error clearing question_bank:', deleteError);
    process.exit(1);
  }
  console.log('Cleared existing questions.');

  // Insert in batches of 50
  const batchSize = 50;
  let inserted = 0;

  for (let i = 0; i < questions.length; i += batchSize) {
    const batch = questions.slice(i, i + batchSize);
    const rows = batch.map(q => ({
      concept_id: q.concept_id,
      type: q.type,
      question_text: q.question_text,
      expected_answer: q.expected_answer,
      difficulty: q.difficulty,
      related_concept_id: q.related_concept_id || null,
      is_active: true,
    }));

    const { error } = await supabase.from('question_bank').insert(rows);

    if (error) {
      console.error(`Error inserting batch starting at ${i}:`, error);
      process.exit(1);
    }

    inserted += batch.length;
    console.log(`  Inserted ${inserted}/${questions.length}`);
  }

  // Print summary by concept
  const byConcept: Record<string, number> = {};
  for (const q of questions) {
    byConcept[q.concept_id] = (byConcept[q.concept_id] || 0) + 1;
  }

  console.log(`\n=== Summary ===`);
  console.log(`Total questions: ${questions.length}`);
  console.log(`Concepts covered: ${Object.keys(byConcept).length}`);
  console.log(`\nQuestions per concept:`);
  for (const [concept, count] of Object.entries(byConcept).sort()) {
    console.log(`  ${concept}: ${count}`);
  }

  // Also seed project_concepts mappings
  console.log(`\n=== Seeding Project-Concept Mappings ===\n`);

  const projectConcepts = [
    // project-kv-store → Phase 1 concepts
    { project_id: 'project-kv-store', concept_id: 'replication' },
    { project_id: 'project-kv-store', concept_id: 'partitioning' },
    { project_id: 'project-kv-store', concept_id: 'distributed-failures' },
    { project_id: 'project-kv-store', concept_id: 'consistency-models' },
    { project_id: 'project-kv-store', concept_id: 'consensus' },

    // project-react-agent → Phase 2 concepts
    { project_id: 'project-react-agent', concept_id: 'react-pattern' },
    { project_id: 'project-react-agent', concept_id: 'tool-use' },
    { project_id: 'project-react-agent', concept_id: 'chain-of-thought' },
    { project_id: 'project-react-agent', concept_id: 'structured-output' },

    // project-rag-system → Phase 3 concepts
    { project_id: 'project-rag-system', concept_id: 'rag-basics' },
    { project_id: 'project-rag-system', concept_id: 'vector-search' },
    { project_id: 'project-rag-system', concept_id: 'chunking-strategies' },
    { project_id: 'project-rag-system', concept_id: 'embeddings' },
    { project_id: 'project-rag-system', concept_id: 'hybrid-search' },

    // project-validators → Phase 4 concepts
    { project_id: 'project-validators', concept_id: 'output-validation' },
    { project_id: 'project-validators', concept_id: 'prompt-injection' },
    { project_id: 'project-validators', concept_id: 'llm-security-owasp' },

    // project-router → Phase 5 concepts
    { project_id: 'project-router', concept_id: 'model-routing' },
    { project_id: 'project-router', concept_id: 'token-economics' },
    { project_id: 'project-router', concept_id: 'semantic-caching' },

    // project-framework-critique → Phase 6 concepts
    { project_id: 'project-framework-critique', concept_id: 'langchain-architecture' },
    { project_id: 'project-framework-critique', concept_id: 'framework-tradeoffs' },
    { project_id: 'project-framework-critique', concept_id: 'minimal-implementations' },
  ];

  // Clear existing
  await supabase
    .from('project_concepts')
    .delete()
    .neq('project_id', '__none__');

  const { error: pcError } = await supabase
    .from('project_concepts')
    .insert(projectConcepts);

  if (pcError) {
    console.error('Error inserting project_concepts:', pcError);
    process.exit(1);
  }

  console.log(`Inserted ${projectConcepts.length} project-concept mappings.`);
  console.log('\n=== Done! ===\n');
}

seedQuestionBank().catch((err) => {
  console.error('Seed failed:', err);
  process.exit(1);
});

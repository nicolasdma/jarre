# DDIA CapÃ­tulo 9: Consistency and Consensus
## SesiÃ³n Pre-Lectura â€” PreparaciÃ³n Conceptual

**Fecha:** 2026-02-15
**Objetivo:** Construir un mapa mental de los conceptos clave ANTES de leer el capÃ­tulo en inglÃ©s.
**MÃ©todo:** ExplicaciÃ³n progresiva con preguntas y respuestas.

---

## Concepto 1: El Problema Fundamental â€” Â¿QuÃ© Pueden Prometer los Sistemas Distribuidos?

En un sistema con una sola mÃ¡quina, las cosas son simples: hay una copia de los datos, un reloj, un orden claro. Cuando distribuyes datos en mÃºltiples nodos, surgen preguntas que no existÃ­an antes:

- Si dos nodos tienen copias distintas, **Â¿cuÃ¡l es la correcta?**
- Si dos escrituras ocurren "al mismo tiempo", **Â¿cuÃ¡l fue primero?**
- Si un nodo se cae a medio camino, **Â¿los demÃ¡s continÃºan o esperan?**

> **Todo el capÃ­tulo 9 trata de: Â¿quÃ© garantÃ­as podemos ofrecer sobre consistencia y orden en un sistema distribuido, y a quÃ© costo?**

**AnalogÃ­a:**
- **Una sola mÃ¡quina** = una persona tomando decisiones sola. Siempre coherente consigo misma.
- **Sistema distribuido** = un comitÃ© de 5 personas en ciudades distintas, comunicÃ¡ndose por cartas. Â¿CÃ³mo se ponen de acuerdo?

El capÃ­tulo presenta un espectro de garantÃ­as, de la mÃ¡s dÃ©bil a la mÃ¡s fuerte:

```
DÃ©bil                                              Fuerte
  â”‚                                                   â”‚
  â–¼                                                   â–¼
Eventual          Causal            Linearizability
Consistency       Consistency       (Consistencia fuerte)
  â”‚                  â”‚                    â”‚
"AlgÃºn dÃ­a         "Respeto el          "Parece una
 convergen"         orden lÃ³gico"        sola copia"
```

Cada nivel mÃ¡s fuerte cuesta mÃ¡s en latencia y disponibilidad. No hay almuerzo gratis.

---

## Concepto 2: Linearizability â€” La GarantÃ­a MÃ¡s Fuerte

Linearizability significa que el sistema **se comporta como si hubiera una sola copia de los datos**, aunque haya mÃºltiples rÃ©plicas. Toda operaciÃ³n aparenta ejecutarse en un instante atÃ³mico entre su invocaciÃ³n y su respuesta.

### Â¿CÃ³mo se ve?

```
Cliente A:  |--- write(x=1) ---|
Cliente B:              |--- read(x) ---| â†’ debe retornar 1

Tiempo â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–º

Si B empieza a leer DESPUÃ‰S de que A completÃ³ la escritura,
B DEBE ver el valor nuevo. No puede ver un valor viejo.
```

### Â¿CuÃ¡ndo lo necesitas?

1. **ElecciÃ³n de lÃ­der:** Si dos nodos creen ser lÃ­der simultÃ¡neamente, tienes split-brain. Necesitas un lock linearizable.
2. **Restricciones de unicidad:** Dos usuarios registrando el mismo username. Solo uno debe ganar.
3. **Saldos bancarios:** No puedes permitir que dos lecturas simultÃ¡neas vean el mismo saldo y ambas autoricen un retiro.

### Â¿CuÃ¡ndo NO lo necesitas?

- Feeds de redes sociales (si ves un post 2 segundos tarde, no pasa nada)
- Analytics y reportes (datos histÃ³ricos, la consistencia eventual basta)
- Caches (por definiciÃ³n, datos potencialmente desactualizados)

### Linearizability vs Serializability â€” La ConfusiÃ³n ClÃ¡sica

| | **Linearizability** | **Serializability** |
|---|---|---|
| **Dominio** | Sistemas distribuidos | Bases de datos (transacciones) |
| **Pregunta** | Â¿Se comporta como una sola copia? | Â¿Las transacciones se comportan como si fueran secuenciales? |
| **Aplica a** | Operaciones individuales de lectura/escritura | Transacciones completas (multi-operaciÃ³n) |
| **CapÃ­tulo** | Cap. 9 | Cap. 7 |

Son conceptos ortogonales. Puedes tener uno sin el otro, ambos, o ninguno.

---

## Concepto 3: El Teorema CAP â€” El Trade-off Inevitable

CAP dice que ante una **particiÃ³n de red** (P), debes elegir entre:

```
        Consistency (C)
            /\
           /  \
          /    \
         / ELIGE \
        / UNO     \
       /____________\
Availability (A) â†â”€â”€â”€ Partition
                      Tolerance (P)
```

- **CP:** Ante particiÃ³n, rechazas solicitudes para mantener consistencia. Ejemplo: ZooKeeper, etcd.
- **AP:** Ante particiÃ³n, sigues respondiendo pero con datos potencialmente desactualizados. Ejemplo: Cassandra, DynamoDB.

> **Cuidado:** CAP es mÃ¡s un eslogan que un teorema riguroso. En la prÃ¡ctica las particiones son raras pero reales, y la decisiÃ³n no es binaria sino un espectro de trade-offs.

**AnalogÃ­a:** Dos sucursales bancarias pierden comunicaciÃ³n.
- **CP:** Ambas dejan de operar hasta restaurar la comunicaciÃ³n. Nadie cobra de mÃ¡s.
- **AP:** Ambas siguen operando con su Ãºltima copia. Riesgo de sobregiro, pero el negocio no para.

---

## Concepto 4: Ordenamiento â€” Causal vs Total

El orden importa. Si dices "cancelar pedido" y luego "confirmar envÃ­o", el resultado depende del orden. En un sistema distribuido, establecer ese orden es difÃ­cil.

### Orden causal

Dos eventos tienen relaciÃ³n causal si uno **pudo haber influido** en el otro. Si no hay relaciÃ³n, son **concurrentes** y su orden no importa.

```
Nodo A: post("Hola")  â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–º
                            â”‚
Nodo B:                     â””â”€â”€â–º reply("Hola, Â¿quÃ© tal?")
                                        â”‚
                                        â–¼
                              La reply DEPENDE del post
                              â†’ hay orden causal

Nodo A: post("Me gusta el cafÃ©")
Nodo C: post("Hoy llueve")
                              â†’ concurrentes, orden irrelevante
```

La consistencia causal es mÃ¡s dÃ©bil que linearizability pero mucho mÃ¡s barata: no necesitas coordinaciÃ³n global, solo rastrear dependencias.

### Orden total

En un orden total, **cualquier par de eventos** se puede comparar: uno fue primero. Linearizability implica orden total. La consistencia causal solo ordena eventos relacionados (orden parcial).

### Lamport Timestamps

Para establecer un orden total sin un reloj centralizado, Lamport propuso un contador lÃ³gico:

```
Nodo A (counter=1)         Nodo B (counter=1)
    â”‚                           â”‚
    â”œâ”€ op1 â†’ (1, A)            â”‚
    â”‚                           â”œâ”€ op2 â†’ (1, B)
    â”‚                           â”‚
    â”‚ â”€â”€â”€ mensaje â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–º   â”‚
    â”‚                           â”œâ”€ recibe: max(1,1)+1 = 2
    â”‚                           â”œâ”€ op3 â†’ (2, B)
    â”‚                           â”‚
    â—„â”€â”€â”€â”€ mensaje â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€    â”‚
    â”œâ”€ recibe: max(1,2)+1 = 3  â”‚
    â”œâ”€ op4 â†’ (3, A)            â”‚

Orden total: (1,A) < (1,B) < (2,B) < (3,A)
  (desempate por ID de nodo cuando el counter es igual)
```

**LimitaciÃ³n:** Lamport timestamps dan orden total **despuÃ©s del hecho**. No puedes usarlos para tomar decisiones en tiempo real (como "Â¿quiÃ©n registrÃ³ este username primero?") porque necesitarÃ­as consultar a TODOS los nodos antes de decidir.

---

## Concepto 5: Total Order Broadcast

Para resolver la limitaciÃ³n de Lamport timestamps, necesitas **total order broadcast**: un protocolo donde todos los nodos reciben los mismos mensajes en el mismo orden.

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Nodo A  â”‚    â”‚  Nodo B  â”‚    â”‚  Nodo C  â”‚
â””â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”˜
     â”‚               â”‚               â”‚
     â”‚   msg1        â”‚               â”‚
     â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–ºâ”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–ºâ”‚
     â”‚               â”‚               â”‚
     â”‚         msg2  â”‚               â”‚
     â”‚â—„â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–ºâ”‚
     â”‚               â”‚               â”‚
     â”‚   msg3        â”‚               â”‚
     â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–ºâ”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–ºâ”‚
     â–¼               â–¼               â–¼
  [msg1,msg2,msg3] [msg1,msg2,msg3] [msg1,msg2,msg3]
  Todos ven el MISMO orden
```

Propiedades:
1. **Reliable delivery:** Si un nodo recibe un mensaje, todos lo reciben.
2. **Totally ordered:** Todos procesan los mensajes en el mismo orden.

> **Total order broadcast es equivalente al consenso.** Si puedes resolver uno, puedes resolver el otro. Esta es una de las ideas mÃ¡s profundas del capÃ­tulo.

---

## Concepto 6: Two-Phase Commit (2PC) â€” Transacciones Distribuidas

Cuando una transacciÃ³n toca mÃºltiples nodos (o mÃºltiples bases de datos), necesitas que TODOS hagan commit o NINGUNO. 2PC resuelve esto con un **coordinador**:

```
                    Coordinador
                        â”‚
              â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
              â–¼         â–¼         â–¼
          Nodo A     Nodo B     Nodo C

FASE 1: "Â¿Pueden hacer commit?"  (PREPARE)
              â”‚         â”‚         â”‚
           "SÃ­" â”€â”€â–º    "SÃ­" â”€â”€â–º  "SÃ­" â”€â”€â–º  Coordinador

FASE 2: "Todos dijeron sÃ­ â†’ COMMIT"
              â”‚         â”‚         â”‚
           COMMIT    COMMIT    COMMIT
```

### El Problema del Coordinador

Si el coordinador se cae **entre la Fase 1 y la Fase 2**, los nodos quedan "in doubt" (en duda):

```
           Coordinador
               ğŸ’¥ (se cae)

          Nodo A          Nodo B
        "Dijo que sÃ­,   "Dijo que sÃ­,
        pero Â¿commit     pero Â¿commit
        o abort?"        o abort?"

         â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
         â”‚ No pueden decidir    â”‚
         â”‚ solos. BLOQUEADOS    â”‚
         â”‚ hasta que el         â”‚
         â”‚ coordinador vuelva.  â”‚
         â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

Los nodos tienen **locks** tomados sobre filas que no pueden liberar. EstÃ¡n paralizados. Esto puede durar minutos, horas, o hasta requerir intervenciÃ³n manual.

> **2PC no es un protocolo de consenso.** Es un protocolo de commit atÃ³mico. Si el coordinador falla, el sistema se bloquea. Un protocolo de consenso real (Raft, Paxos) puede progresar mientras haya mayorÃ­a.

---

## Concepto 7: Consenso â€” Ponerse de Acuerdo a Pesar de Fallos

Consenso es que un grupo de nodos se ponga de acuerdo en un valor, incluso si algunos nodos fallan. Es EL problema central de los sistemas distribuidos.

### Propiedades del consenso

1. **Uniform agreement:** Todos los nodos que deciden eligen el mismo valor.
2. **Integrity:** NingÃºn nodo decide dos veces.
3. **Validity:** El valor decidido fue propuesto por algÃºn nodo.
4. **Termination:** Todo nodo que no falle eventualmente decide.

### FLP Impossibility

Fischer, Lynch y Paterson demostraron (1985) que en un sistema asÃ­ncrono con incluso **un solo nodo que pueda fallar**, no existe algoritmo de consenso que siempre termine.

En la prÃ¡ctica, los algoritmos reales (Raft, Paxos) evitan esta imposibilidad usando **timeouts** y **detecciÃ³n de fallos imperfecta** â€” sacrifican la garantÃ­a teÃ³rica de terminaciÃ³n en todos los casos a cambio de funcionar en la prÃ¡ctica.

### Raft vs Paxos

| | **Paxos** | **Raft** |
|---|---|---|
| **AÃ±o** | 1989 (Lamport) | 2014 (Ongaro & Ousterhout) |
| **DiseÃ±o** | Elegante pero difÃ­cil de implementar | DiseÃ±ado para ser entendible |
| **LÃ­der** | Multi-Paxos usa lÃ­der, Paxos bÃ¡sico no | Siempre hay un lÃ­der elegido |
| **Complejidad** | Muy alta (famoso por su dificultad) | Moderada (paper pensado como tutorial) |
| **Usado por** | Google Chubby, algunos sistemas legacy | etcd, CockroachDB, TiKV |
| **Idea central** | Proponer â†’ Prometer â†’ Aceptar | Elegir lÃ­der â†’ lÃ­der replica log |

Ambos requieren **mayorÃ­a** (quÃ³rum) para progresar. Con 5 nodos, toleran la falla de 2. Con 3 nodos, toleran 1.

```
Raft â€” Flujo simplificado:

  1. ELECCIÃ“N: Los nodos eligen un lÃ­der por votaciÃ³n

     Nodo A â”€â”€â”€voteâ”€â”€â”€â–º Nodo B (candidato)
     Nodo C â”€â”€â”€voteâ”€â”€â”€â–º Nodo B (candidato)
                         â”‚
                    Nodo B es LÃDER (2 de 3 votos)

  2. REPLICACIÃ“N: El lÃ­der recibe escrituras y las replica

     Cliente â”€â”€writeâ”€â”€â–º LÃ­der B
                         â”‚
                    â”Œâ”€â”€â”€â”€â”¼â”€â”€â”€â”€â”
                    â–¼         â–¼
                 Nodo A    Nodo C
                 (replica)  (replica)
                    â”‚         â”‚
                   ACK       ACK
                    â”‚         â”‚
                    â””â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”˜
                         â”‚
                    MayorÃ­a confirmÃ³ â†’ COMMIT
```

---

## Concepto 8: Servicios de CoordinaciÃ³n â€” ZooKeeper, etcd

En la prÃ¡ctica, no implementas consenso tÃº mismo. Usas un servicio dedicado:

| | **ZooKeeper** | **etcd** |
|---|---|---|
| **Protocolo** | Zab (similar a Paxos) | Raft |
| **Lenguaje** | Java | Go |
| **Usado por** | Kafka, HBase, Hadoop | Kubernetes, CockroachDB |
| **API** | JerÃ¡rquica (como filesystem) | Key-value plano |

### Â¿Para quÃ© los usas?

1. **ElecciÃ³n de lÃ­der:** "Â¿QuiÃ©n es el lÃ­der del cluster de Kafka?" Un nodo crea un nodo efÃ­mero en ZooKeeper. Si ese nodo muere, ZooKeeper lo detecta y otro toma el liderazgo.

2. **ConfiguraciÃ³n distribuida:** Todos los nodos leen la config del mismo lugar. Si cambia, ZooKeeper notifica a todos.

3. **Service discovery:** "Â¿En quÃ© IP estÃ¡ el servicio X?" Los servicios se registran al arrancar.

4. **Locks distribuidos:** ExclusiÃ³n mutua entre procesos en distintas mÃ¡quinas.

> **Regla de oro:** Tu aplicaciÃ³n NO deberÃ­a usar consenso en el hot path (cada request). Usa un servicio de coordinaciÃ³n para decisiones infrecuentes pero crÃ­ticas (elecciÃ³n de lÃ­der, cambios de configuraciÃ³n), y replica el resultado.

```
Tu aplicaciÃ³n (miles de requests/segundo)
       â”‚
       â”‚  "Â¿QuiÃ©n es el lÃ­der?"
       â”‚  (pregunta infrecuente)
       â–¼
  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
  â”‚ ZooKeeper â”‚  â† 3-5 nodos con consenso
  â”‚  / etcd   â”‚     (lento pero correcto)
  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
       â”‚
       â”‚  "Nodo B es lÃ­der"
       â”‚  (respuesta cacheada)
       â–¼
  Nodo B procesa requests directamente
  (rÃ¡pido, sin consenso por request)
```

---

## Resumen Visual

```
                  Consistency & Consensus
                          â”‚
          â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
          â”‚               â”‚               â”‚
     GarantÃ­as         Orden          Acuerdo
          â”‚               â”‚               â”‚
   â”Œâ”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”   â”Œâ”€â”€â”€â”¼â”€â”€â”€â”     â”Œâ”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”
   â”‚      â”‚      â”‚   â”‚       â”‚     â”‚           â”‚
Eventual Causal Linear. Lamport  Total    Transacc. Consenso
   â”‚      â”‚      â”‚   Timestamps Order     Distrib.    â”‚
   â”‚      â”‚      â”‚       â”‚     Broadcast    â”‚     â”Œâ”€â”€â”€â”¼â”€â”€â”€â”
  DÃ©bil  Media  Fuerte   â”‚        â”‚        2PC   Raft  Paxos
   â”‚      â”‚      â”‚       â””â”€â”€â”€â”¬â”€â”€â”€â”€â”˜         â”‚      â”‚
  Gratis  â”‚    Costosa    Equivalente    Bloquea  MayorÃ­a
          â”‚      â”‚        al consenso    si coord.  â”‚
          â”‚    CAP                        falla   ZooKeeper
          â”‚   (CP vs AP)                          etcd
          â”‚
     Solo rastrear
     dependencias
```

---

## TÃ©rminos Clave para el CapÃ­tulo (InglÃ©s â†’ EspaÃ±ol)

| InglÃ©s | EspaÃ±ol | QuÃ© es |
|--------|---------|--------|
| Linearizability | Linearizabilidad | GarantÃ­a de que el sistema actÃºa como una sola copia |
| Eventual consistency | Consistencia eventual | Las rÃ©plicas convergen eventualmente, sin garantÃ­a de cuÃ¡ndo |
| Causal consistency | Consistencia causal | Respeta el orden causa-efecto entre operaciones |
| Total order broadcast | DifusiÃ³n con orden total | Todos los nodos reciben mensajes en el mismo orden |
| Lamport timestamp | Marca de tiempo de Lamport | Reloj lÃ³gico para establecer orden total |
| Consensus | Consenso | MÃºltiples nodos acuerdan un valor a pesar de fallos |
| Two-phase commit (2PC) | Commit en dos fases | Protocolo para transacciones atÃ³micas distribuidas |
| Coordinator | Coordinador | Nodo central que dirige el 2PC |
| Split-brain | Cerebro dividido | Dos nodos creen ser lÃ­der simultÃ¡neamente |
| Quorum | QuÃ³rum | MayorÃ­a de nodos necesaria para tomar decisiones |
| Epoch / term | Ã‰poca / mandato | PerÃ­odo de liderazgo en un protocolo de consenso |
| FLP impossibility | Imposibilidad FLP | No existe consenso determinista en sistemas asÃ­ncronos con fallos |
| Fencing token | Token de cercado | NÃºmero monotÃ³nico para invalidar locks expirados |
| Service discovery | Descubrimiento de servicios | Encontrar la direcciÃ³n de un servicio en el cluster |

---

## Preguntas de ComprensiÃ³n (Auto-evaluaciÃ³n)

1. Â¿CuÃ¡l es la diferencia entre linearizability y serializability? Â¿Por quÃ© se confunden?
2. Â¿Por quÃ© la consistencia causal es mÃ¡s barata que linearizability?
3. Explica el teorema CAP con tus palabras. Â¿Por quÃ© decimos que es mÃ¡s un eslogan que un teorema riguroso?
4. Â¿QuÃ© limitaciÃ³n tienen los Lamport timestamps que total order broadcast resuelve?
5. Â¿QuÃ© pasa en 2PC si el coordinador se cae despuÃ©s de enviar PREPARE pero antes de enviar COMMIT?
6. Â¿Por quÃ© 2PC NO es un protocolo de consenso?
7. Â¿QuÃ© dice la imposibilidad FLP y cÃ³mo la evitan Raft y Paxos en la prÃ¡ctica?
8. Â¿CuÃ¡l es la diferencia principal de diseÃ±o entre Raft y Paxos?
9. Â¿Por quÃ© no deberÃ­as usar consenso en el hot path de tu aplicaciÃ³n?
10. Â¿QuÃ© problemas resuelve ZooKeeper/etcd y por quÃ© no los resuelves tÃº mismo?
